# Kafka Multi-Terminal Lab: Full System Documentation

This document provides a comprehensive overview of the Kafka environment configured across six terminals, demonstrating both **Load Balancing** (Queueing) and **Fan-out** (Publish/Subscribe) architectures.


---

## ðŸ—ï¸ System Architecture



| Terminal | Process Role | Topic | Group Name | Strategy |
| :--- | :--- | :--- | :--- | :--- |
| **Terminal 1** | Infrastructure | N/A | N/A | **Broker/Coordinator** |
| **Terminal 2** | Consumer Instance 1 | `queue-test` | `app1` | **Load Balancing** |
| **Terminal 3** | Consumer Instance 2 | `queue-test` | `app1` | **Load Balancing** |
| **Terminal 4** | Consumer Instance 3 | `topic-test` | `app2` | **Fan-out (Sub A)** |
| **Terminal 5** | Consumer Instance 4 | `topic-test` | `app3` | **Fan-out (Sub B)** |
| **Terminal 6** | Producer | `topic-test` | N/A | **Data Entry** |

---

## ðŸ’» Terminal-by-Terminal Commands

### Terminal 1: Infrastructure & Topics
This terminal hosts the server processes and the initial topic definitions.
```bash
# 1. Start Zookeeper (The Manager)
/app/confluent-7.9.1/bin/zookeeper-server-start /app/confluent-7.9.1/etc/kafka/zookeeper.properties >/dev/null&

# 2. Start Kafka Broker (The Storage Engine)
/app/confluent-7.9.1/bin/kafka-server-start /app/confluent-7.9.1/etc/kafka/server.properties >/dev/null&

# 3. Create Topics (2 Partitions each)
/app/confluent-7.9.1/bin/kafka-topics --bootstrap-server localhost:9092 --create --topic queue-test --partitions 2 --replication-factor 1
/app/confluent-7.9.1/bin/kafka-topics --bootstrap-server localhost:9092 --create --topic topic-test --partitions 2 --replication-factor 1
```

### Terminal 2: Consumer Group app1 (Instance 1)
Designed to handle half the load of the `queue-test` topic.

```bash
export TOPIC_NAME=queue-test
export CONSUMER_GROUP_NAME=app1
java -cp ".:/opt/kafka/libs/*" com.example.KafkaConsumerExample
```

### Terminal 3: Consumer Group app1 (Instance 2)
Designed to handle the other half of the load of the queue-test topic.
```bash
export TOPIC_NAME=queue-test
export CONSUMER_GROUP_NAME=app1
java -cp ".:/opt/kafka/libs/*" com.example.KafkaConsumerExample
```

### Terminal 4: Consumer Group app2
An independent subscriber listening to topic-test. It receives every message sent to this topic.

```
export TOPIC_NAME=topic-test
export CONSUMER_GROUP_NAME=app2
java -cp ".:/opt/kafka/libs/*" com.example.KafkaConsumerExample
```
### Terminal 5: Consumer Group app3
A second independent subscriber listening to topic-test. It also receives every message sent to this topic.

```
export TOPIC_NAME=topic-test
export CONSUMER_GROUP_NAME=app3
java -cp ".:/opt/kafka/libs/*" com.example.KafkaConsumerExample
```

### Terminal 6: Keyed Producer
This terminal is used to manually send messages into the system.

```
./kafka-console-producer --broker-list localhost:9092 --topic topic-test --property "parse.key=true" --property "key.separator=:"
# Input format: KEY:VALUE (e.g., user1:login_event)
```
### Load Balancing (Terminals 2 & 3)
- Pattern: One-to-One (Queueing) 
- How it works: When you put multiple consumers in the same group (e.g., app1), Kafka treats them as a single logical unit. 
- It divides the topic's partitions among the members. Because queue-test has 2 partitions, Terminal 2 gets Partition 0 and Terminal 3 gets Partition 1.
- __Mechanism:__ Each message is processed by exactly one consumer in the group.
- __Scalability:__ If the volume of messages increases, you can add more terminals (up to the number of partitions) to speed up processing.
-  __Use Case:__ Order Processing: Imagine an e-commerce site where thousands of orders are placed per second. You don't want one server to do all the work, and you definitely don't want two servers to accidentally charge the same credit card twice. Load balancing ensures each order is handled once, but the work is shared.

### Fan-out / Pub-Sub (Terminals 4 & 5)
- Pattern: One-to-Many (Broadcasting) 
- How it works: When you give consumers different group names (e.g., app2 and app3), Kafka treats them as completely separate applications. Each group gets its own "offset" (a bookmark of where it stopped reading).
- __Mechanism:__ Every message is sent to every group. Terminal 4 and Terminal 5 both get 100% of the data.
- __Independence:__ If Terminal 4 crashes, it has no effect on Terminal 5.
- __Use Case:__ System-Wide Notifications. Imagine a __"New User Signed Up"__ event. 
    - Service A (Group app2) takes that event and sends a Welcome Email. 
    - Service B (Group app3) takes that same event and updates an Analytics Dashboard.

Both need the same data at the same time for different purposes.

### ðŸŽ“ Key Concepts Demonstrated
1. Load Balancing (T2 & T3)
When multiple consumers share the same CONSUMER_GROUP_NAME, Kafka treats them as a team. Since queue-test has 2 partitions, Kafka assigns Partition 0 to Terminal 2 and Partition 1 to Terminal 3.

2. Fan-out / Pub-Sub (T4 & T5)
Because Terminal 4 and Terminal 5 have different group names (app2 vs app3), they do not share the load. Instead, they each get a private copy of the data stream. This is ideal for microservices where multiple apps need the same data (e.g., an Analytics service and an Email service).

3. Keyed Message Routing
By using key:value in Terminal 6, Kafka hashes the key to decide which partition the message goes to. This ensures that all messages for the same key (like the same User ID) are processed in order by the same consumer.

### kafka-consumer-groups tool.
- To see exactly how Kafka has distributed the partitions among your active terminals, you use the kafka-consumer-groups tool. This is the best way to verify that your `Load Balancing` (Terminal 2 & 3) and `Fan-out` (Terminal 4 & 5) setups are working as intended

Open a 7th Terminal (or use Terminal 1) and run these commands:
1. __Check the Load Balanced Group (app1):__
This will show you how Terminal 2 and Terminal 3 have split the work for queue-test.

```
app/confluent-7.9.1/bin/kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group app1
# if you don't provide --group you can see all groups app1, app2, app3
```

__What to look for in the output:__
- You should see two rows (one for Partition 0 and one for Partition 1)
- Look at the CONSUMER-ID and HOST columns. You will see two different IDs, confirming that Terminal 2 (app1) and Terminal 3(app1) are each handling one specific partition.
2. __Check the Fan-out Groups (app2 or app3):__ 
Run this to see how a single consumer handles an entire topic.
```
/app/confluent-7.9.1/bin/kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group app2
```
__What to look for in the output:__
- You will still see two rows (Partition 0 and 1).
- However, both rows will have the same CONSUMER-ID. This confirms that Terminal 4 is solely responsible for the entire topic because it is the only member of that group.

__Understanding the Output Columns__

When you run the commands above, you will see a table. Here is what the columns mean:
|Column|Meaning|
|------|:-------|
|PARTITION | The specific "lane" of the topic being read.|
|CURRENT-OFFSET|The last message number the consumer successfully read.|
|LOG-END-OFFSET|The last message number that exists in the Kafka broker.|
|LAG|The number of messages waiting to be processed (LOG-END-OFFSET minus CURRENT-OFFSET).|
|CONSUMER-ID|The unique name Kafka gave to your specific Terminal window.|

#### The "Rebalance" Test
-If you want to see Kafka's intelligence in action:
- Keep the describe command ready in your 7th terminal.
- Kill Terminal 2 (Ctrl+C).
- Run the describe command for app1 again immediately.
- You will see that Terminal 3 has automatically taken over both Partition 0 and Partition 1. This is "Rebalancing".

---
### Delayed Message delivery:
1. identify __LAG__ using `kafka-consumer-groups` for say --group app1/app2/app3
    -  __Lag is high on all partitions:__ consumer logic is likely too slow (e.g., complex database writes or API calls). You need to optimize the code or add more consumers to the group.
    - __Lag is high on only ONE partition:__ You have a Data Skew problem. Your producer is sending too much data to one specific key, overwhelming the one terminal assigned to that partition. `hot key`
        1. The Immediate Fix: **Increase Partitions**: 
            - If you have more consumers than partitions, some consumers will do nothing.
            - Increasing the partition count, you give Kafka more "buckets" to distribute the keys into. It redistributes the data. Even if one key is still heavy, other keys that were previously stuck in the same partition might move to a new, quieter partition.   `kafka-topics --alter --topic <topic> --partitions <new_number>` 
            - It does not solve **One Single Hot Key** as it will still land in same bucket
   WARNING: Once you increase partitions, you cannot decrease them.
        2. **Remove Keys:** If ordering isn't required, send messages with a null key to enable Round-Robin distribution.
        3. **Key Salting:** Suffix heavy keys with a random integer (e.g., `key_1`, `key_2`) to force distribution across partitions.
2. __Lag is 0 but you still feel a delay:__ The delay is likely on the Producer side (it's buffering) or Network latency, not the Consumer.
    - Resetting the Offset: If your consumers fall so far behind that you just want to "skip" the old messages and jump to the newest ones, you can use the same tool to reset the bookmark:
         ``` 
            # Note: You must stop your Java consumers before running this
            /app/confluent-7.9.1/bin/kafka-consumer-groups \
            --bootstrap-server localhost:9092 \
            --group app1 \
            --topic queue-test \
            --reset-offsets --to-latest --execute
        ```
---
### The "Overcrowded Bucket"
- Imagine you have 100 unique users and only 2 partitions. 
    - The Math: On average, 50 users are mapped to Partition 0, and 50 users are mapped to Partition 1.
    - The Problem: If Partition 0's consumer (Terminal 2) is lagging, it's not because of one user; it's because the combined volume of those 50 users is too much for one terminal to handle.
- **How Increasing Partitions Solves This:** When you increase the partitions from 2 to 10:
    - Redistribution: The 100 users are now spread across 10 partitions (~10 users per partition).
    - Parallelism: You can now spin up 10 Terminal windows (Consumers) in Group app1.
    - Reduced Load: Each terminal now only has to process the data for 10 users instead of 50. The "Total Lag" drops because you have more "workers" doing smaller pieces of the work.
## âš ï¸ Production Warning: Altering Partitions

Increasing partitions is a powerful scaling tool but carries risks:
1. **Ordering:** Changes the key-to-partition mapping. If key-ordering is vital, do not alter partitions while the system is under heavy load.
2. **Immutability:** Partition count can only go UP, never DOWN.
3. **Downstream Load:** Ensure your database/API can handle the increased parallelism of extra consumers.

**Recommendation:** Always start production topics with a partition count that is a multiple of your expected consumer count (e.g., 6, 12, or 24).

- Here are the three critical things that happen in a production environment when you run that --alter command:
    1. The "Order" Shuffling Problem:  (Biggest Risk) Kafka guarantees message ordering per partition. When you increase the number of partitions, the hashing formula changes because the number of partitions is now larger. 
        - Before: User_A =>  Partition 1. 
        - After: User_A => Partition 3.
        - If you have old messages for User_A still sitting in Partition 1, and new messages start flowing into Partition 3, your consumer might process the new data before the old data.
        - If your application relies on strict chronological order for a specific key, increasing partitions in prod will break that order temporarily.
    2. The Consumer Rebalance "Stop-the-World": The moment you increase partitions, the Kafka Broker notifies the Consumer Group. 
        - The consumers will stop reading for a few seconds.
        - They perform a Rebalance to figure out who owns the new partitions.- In a high-traffic production environment, this "hiccup" can cause a temporary spike in lag while the consumers get organized.
    3. Downstream Impact (Database/API): If you increase partitions from 2 to 20 so you can run 20 consumers, you are effectively 10x-ing the load on your downstream systems (like your Database or a 3rd party API).
        - The Trap: Your Kafka consumers might handle the speed fine, but your Database might crash because it suddenly has 20 simultaneous connections writing data instead of 2.

- âœ… Best Practices for Prod: If you must increase partitions in production, follow this checklist:
    1. Over-provision Early: It is better to start a topic with more partitions than you need (e.g., 6 or 12) even if you only have 1 consumer today. This gives you "headroom" to scale up consumers without altering the topic later.
    2. Drain the Lag First: Try to increase partitions when lag is low.Check the Key: If you are using null keys (Round Robin), increasing partitions is 100% safe because order was never guaranteed anyway.
---
## ðŸ§® The Hashing Logic

Kafka determines partition placement using:
`abs(Murmur2(key)) % num_partitions`

### Why $N$ Matters:
Because the partition index depends on the total number of partitions ($N$), increasing $N$ changes the destination for existing keys. 

This formula is exactly why increasing partitions in production can be risky for keyed data.
- Imagine you have a key order_99 and 2 partitions:
    - Murmur2("order_99") = 14567831456783 mod 2 = 1 (The message goes to Partition 1).
- If you increase partitions to 3: 
    - Murmur2("order_99") = 14567831456783 mod 3 = 2 (The message now goes to Partition 2).
    - The Result: The same key just "jumped" to a different lane. If your consumer for Partition 1 was still busy processing old messages for order_99, and the consumer for Partition 2 starts processing the new messages immediately, your data will be processed out of order.

**Pro-Tip:**  If you require strict ordering for a specific key, do not change the partition count unless the topic is empty or you can handle temporary out-of-order processing.

---
## Split-brain

In Kafka, a network partition (often called a "split-brain" scenario) occurs when a consumer is still running but can no longer communicate with the Kafka brokers.

When this happens, the behavior is governed by the Consumer Group Protocol. Here is the step-by-step breakdown of what occurs:

1. **The Missing Heartbeat:** 
    - Every consumer in a group (like your app1) sends a periodic "heartbeat" to the Kafka Broker (specifically the Group Coordinator)
    - If the network is cut, the broker stops receiving these heartbeats. The broker waits for a specific timeout period, known as the `session.timeout.ms` (default is usually 45 seconds).

2. **The Group Rebalance**
    - Once the timeout expires, the Broker decides the consumer is "dead."
    - Rebalancing: The Broker triggers a rebalance.
    - Reassignment: The partitions that were assigned to the "partitioned" consumer are taken away and given to the healthy consumers in the group.
    - Example: If Terminal 2 is cut off, Terminal 3 will suddenly start receiving data for both Partition 0 and Partition 1.
3. **The "Zombie" Consumer Problem**
    - This is the most dangerous part. The consumer that is cut off doesn't know it has been kicked out of the group yet because it can't talk to the broker.
    - Zombie State: The partitioned consumer might continue processing the last batch of messages it downloaded before the network failed.
    - Duplicate Processing: While the "Zombie" is still processing old data, the new consumer (Terminal 3) is already processing the new data for that same partition.
    - The Offset Clash: When the network partition is fixed and the "Zombie" tries to commit its progress (offsets) to Kafka, the Broker will reject it with a CommitFailedException. Kafka knows that the "Zombie" is no longer the rightful owner of that partition.

4. How to Resolve/Mitigate
- In a production environment, we use two main settings to handle this:
    1. `max.poll.interval.ms:` This ensures that if a consumer's code gets "stuck" processing a heavy message (even if the network is fine), it eventually leaves the group so someone else can take over.
    2. `Idempotent Processing:` Since network partitions can cause a message to be processed twice (once by the Zombie and once by the new owner), your application should be able to handle the same message twice without causing errors (e.g., checking if a database record already exists before inserting).

---
### The Timeline of a Double Process
Imagine a consumer in Terminal 2 gets a message: Order #99 - Charge $100.

1. The Fetch: Terminal 2 pulls the message.
2. The Partition: The network cuts. Terminal 2 is now a "Zombie."
3. The Logic (The First Process): Your Java code executes creditCard.charge(100). This happens locally in your app. The money is gone.
54. The Rebalance: The Broker notices Terminal 2 is missing. it gives the same partition to Terminal 3.
5. The Second Process: Terminal 3 pulls the same Order #99. It hasn't been "marked as done" in Kafka yet because the Zombie hasn't committed.
    - Terminal 3 also executes creditCard.charge(100). The customer is now charged twice.
6. The Zombie Returns: Terminal 2 regains network and says "Hey Kafka, I finished Order #99, mark it as done!"
7. The Fencing: The Broker says "CommitFailedException! You aren't the owner anymore."
8. The Result: Even though the commit failed (Step 7), the side effect in the real world (Step 3 and Step 5) already happened twice.

**How Idempotent Processing Saves You**
- To fix this, your application logic needs to be "smart" so that Step 5 doesn't actually do anything. You can achieve this using a Unique Identifier (like the Order ID).

**The Idempotent Logic Flow:**
1. Terminal 3 pulls Order #99.
    - The Check: Terminal 3 checks the Database: SELECT * FROM processed_orders WHERE order_id = '99'. 
    - The Decision: If the record exists (because the Zombie wrote to the DB before the network cut), Terminal 3 skips the charge.
    - If the record doesn't exist, Terminal 3 charges the card and inserts the record.
---
## ðŸ”‘ Key Comparison

| Feature | Producer Key (Kafka Key) | Idempotency Key (Business Key) |
| :--- | :--- | :--- |
| **Primary Scope** | Kafka Broker / Infrastructure | Application Database / Logic |
| **Main Job** | Partitioning & Ordering | Preventing Duplicates |
| **Handled By** | Kafka Partitioner (Murmur2) | Your Java Code (`if exists...`) |
| **Typical Value** | Entity Group (e.g., `user_123`) | Unique Event (e.g., `evt_abc_789`) |